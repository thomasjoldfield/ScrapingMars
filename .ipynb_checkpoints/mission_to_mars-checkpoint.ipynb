{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape latest news stories from NASA mars site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So dependant\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our Mongo on\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mongo is but collection in database of life....\n",
    "db = client.Mars_db\n",
    "collection = db.mars_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's start with our nasa news\n",
    "url = \"https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest\"\n",
    "\n",
    "mars_news = requests.get(url)\n",
    "soup = BeautifulSoup(mars_news.text, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's go grab those articles out of lists, then we roll\n",
    "nasa_articles = soup.find_all(\"div\", class_=\"slide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nasa_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in nasa_articles:\n",
    "    article_title = article.find(\"div\", class_=\"content_title\").text\n",
    "    article_content = article.find(\"div\", class_=\"rollover_description_inner\").text\n",
    "    mars_news = {\n",
    "        \"title\" : article_title,\n",
    "        \"content\" : article_content\n",
    "    }\n",
    "    print(\"----------------\")\n",
    "    print(article_title)\n",
    "    print(article_content)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stick it all into Mongo\n",
    "#Collect it all up\n",
    "#collection.insert_one(mars_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ok, let's use splinter.\n",
    "#Get current featured image url from here:\n",
    "#https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.click_link_by_partial_text('FULL IMAGE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_html = browser.html\n",
    "image_soup = BeautifulSoup(image_html, 'html.parser')\n",
    "\n",
    "image_ext = image_soup.find('img', {'class': 'fancybox-image'})['src']\n",
    "\n",
    "featured_image_url = \"https://www.jpl.nasa.gov\" + image_ext\n",
    "#image_link = image_soup.find(\"div\", class_=\"fancybox-inner\")\n",
    "print(featured_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ok! On to Twitter!\n",
    "#Could scrape like the others, but we learned tweepy. Let's refresh.\n",
    "import tweepy\n",
    "\n",
    "from config import api_twitter\n",
    "from config import api_twitter_secret\n",
    "from config import api_access_token\n",
    "from config import api_access_token_secret\n",
    "# Your Twitter API Keys\n",
    "consumer_key = api_twitter\n",
    "consumer_secret = api_twitter_secret\n",
    "access_token = api_access_token\n",
    "access_token_secret = api_access_token_secret\n",
    "\n",
    "# Setup Tweepy API Authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = api.user_timeline(\"@MarsWxReport\", count = 1)\n",
    "print(tweets[0][\"text\"])\n",
    "mars_weather = tweets[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape mars facts as a table https://space-facts.com/mars/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wow, I think this is the furthest I've ever gotten into a script without importing Pandas.\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_url = \"https://space-facts.com/mars/\"\n",
    "mars_facts = pd.read_html(table_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#damn. That one is so nice and easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ok, finally, images and hemispheres\n",
    "#USGS site here: https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\n",
    "#I think we go back to splinter, so we can do a nice move between the pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere_url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "\n",
    "hemispheres = [\"Cerberus\", \"Schiaperell\", \"Syrtis\", \"Valles\"]\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "for hemisphere in hemispheres:\n",
    "    browser.visit(hemisphere_url)\n",
    "    browser.click_link_by_partial_text(hemisphere)\n",
    "    hem_html = browser.html\n",
    "    hem_soup = BeautifulSoup(hem_html, 'html.parser')\n",
    "    hem_title = hem_soup.find('h2', class_=\"title\")[\"text\"]\n",
    "    hem_downloads = hem_soup.find('div', class_=\"downloads\")\n",
    "    hem_url = hem_downloads.find('a')[\"href\"]\n",
    "    hem_dict = {\n",
    "        \"title\":hem_title,\n",
    "        \"image\":hem_url\n",
    "    }\n",
    "    hemisphere_image_urls.append(hem_dict)\n",
    "    print(\"{} added successfully.\".format(hem_title))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
